{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pH5nqDYSSREn"},"outputs":[],"source":["import os\n","import pickle\n","import torch\n","import sys\n","\n","from PIL import Image\n","from skimage.io import imread\n","from skimage.transform import resize\n","from matplotlib import pyplot as plt\n","from tabulate import tabulate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1950,"status":"ok","timestamp":1639995977387,"user":{"displayName":"thor thorsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07172622034323773422"},"user_tz":-60},"id":"MwWcjokwiLXP","outputId":"7fc01aba-dc8f-4bde-cf34-7264becc9ba1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# fix the path\n","original_path = os.getcwd()\n","sys.path.append(os.path.join('.', '..'))\n","sys.path.append('/content/drive/My Drive/Deep_Learning_Project12/')\n","os.chdir(sys.path[-1])"]},{"cell_type":"markdown","metadata":{"id":"PT_qAUOWYZ7_"},"source":["# Import Data and Wrangling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1100,"status":"ok","timestamp":1639995978484,"user":{"displayName":"thor thorsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07172622034323773422"},"user_tz":-60},"id":"8bTtSqvsijGj","outputId":"577a7494-17e2-4991-9827-edbc101b72b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","data_files = os.listdir(\"Files\")\n","  \n","labels = pd.read_csv(\"Files/dermx_labels.csv\")\n","labels[\"image_path\"] = [os.path.join(os.getcwd(),\"Files\", \"images\", f\"{x}.jpeg\") for x in labels[\"image_id\"]]\n","labels.drop(columns = \"Unnamed: 0\", inplace = True)\n","\n","labels.dropna().reset_index(drop = True)\n","labels = pd.get_dummies(labels, columns = [\"area\"])\n","labels[\"open_comedo\"] = (labels[\"open_comedo\"] > 0).astype(int)\n","\n","features_target = pd.read_csv(\"Files/diseases_characteristics.csv\")\n","features_target.rename(columns={\"Unnamed: 0\":\"disease\"},inplace=True)\n","\n","# create on_hot for diagnosis and get features\n","one_hot = pd.get_dummies(labels[\"diagnosis\"])\n","one_hot_encoding = [list(x) for x in one_hot.values]\n","\n","labels[\"ts\"] = one_hot_encoding\n","\n","# get features as multi hot\n","features_touse = list(labels.columns[list(range(2,9)) + [10,11,12,13]])\n","labels[\"features\"] = labels.loc[:, features_touse].values.tolist()\n","\n","# map feature sequences to value\n","features_map = {}\n","for idx, feat in enumerate(labels[\"features\"].apply(tuple).unique()):\n","  features_map[str(feat)] = idx\n","\n","labels[\"features_label\"] = labels[\"features\"].apply(tuple).apply(str).map(features_map)\n","\n","# get domain\n","domain = pd.read_csv(\"Files/diseases_characteristics.csv\")\n","domain.rename(columns={\"Unnamed: 0\":\"diagnosis\"},inplace=True)\n","domain = pd.get_dummies(domain, columns = [\"area\"])\n","same_sort = [\"diagnosis\"] + features_touse\n","domain = domain[same_sort]  # same sorting\n","\n","domain_one_hot = pd.get_dummies(domain[\"diagnosis\"])\n","\n","domain_one_hot_encoding = [list(x) for x in domain_one_hot.values]\n","domain[\"ts\"] = domain_one_hot_encoding\n","feature_cols = domain.columns[1:12]\n","domain[\"features\"] = domain.loc[:,feature_cols].values.tolist()\n","\n","# add domain features (domain knowledge) to dataframe\n","tf = []\n","for i, row in labels.iterrows():\n","  disease = row[\"diagnosis\"]\n","  true_features = domain.loc[domain.diagnosis == disease].features.tolist()[0]\n","  tf.append(true_features)\n","labels[\"domain_features\"] = tf \n","\n","domain = domain.sort_values(by=\"diagnosis\").reset_index(drop=True)\n","\n","data = labels.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oubdcTaB4J0d"},"outputs":[],"source":["#@title Some helpful functions\n","from HelperFunctions.project_utils import Tracker\n","from sklearn.utils import class_weight\n","import ast\n","\n","def add_no_match(df: pd.DataFrame):\n","  \n","  unique_data = [list(x) for x in set(tuple(x) for x in df.domain_features)]\n","\n","  app = []\n","  for i, row in df.iterrows():\n","    for x in unique_data:\n","      tmp_row = row.copy()\n","      if row[\"domain_features\"] == x:\n","        pass\n","      else:\n","        tmp_row[\"diagnosis\"] = \"no_match\"\n","        tmp_row[\"domain_features\"] = x\n","        app.append(tmp_row)\n","\n","  # Create new data frame\n","  updated_df=df.append(app,ignore_index=True)\n","  \n","  # Update targets \"ts\"\n","  updated_df.drop(columns=\"ts\")\n","  new_dummies = pd.get_dummies(updated_df[\"diagnosis\"])\n","  new_dummies = [list(x) for x in new_dummies.values]\n","  updated_df[\"ts\"] = new_dummies\n","\n","  return updated_df\n","\n","def unique_lists(data: list):\n","  return [list(x) for x in set(tuple(x) for x in data)]\n","\n","def map_domain_knowledge(df: pd.DataFrame):\n","  keys = df.diagnosis.unique().tolist()\n","  map = dict()\n","  for k in keys:\n","    map[k] = df.loc[data[\"diagnosis\"] == k].domain_features.tolist()[0]\n","  return map\n","\n","def calc_multiclass_weights(df: pd.DataFrame, device):\n","  \n","  cls = sorted(df.diagnosis.unique())\n","  y = df.diagnosis.to_list()\n","  csw = class_weight.compute_class_weight('balanced', classes = cls, y = y)\n","  class_weights = torch.tensor(csw,dtype=torch.float).to(device)\n","\n","  return class_weights\n","\n","def calc_multilabel_weights(df: pd.DataFrame, device):\n","  ones_count = np.vstack(df[\"features\"]).sum(axis = 0)\n","  zero_count = len(df) - ones_count\n","  feature_weights = zero_count/ones_count\n","  feature_weights = torch.tensor(feature_weights).to(device)\n","  \n","  return feature_weights\n","\n","\n","\n","def save_splits(skf, x, y):\n","  train_idxs=[]\n","  test_idxs=[]\n","  for train_idx, test_idx in skf.split(x,y):\n","    train_idxs.append(train_idx.tolist())\n","    test_idxs.append(test_idx.tolist())\n","\n","  splits = pd.DataFrame()\n","  splits[\"train\"] = train_idxs\n","  splits[\"test\"] = test_idxs\n","\n","  save_name = \"K_fold/splits.csv\"\n","  with open(save_name,'w') as f:\n","    splits.to_csv(f)\n","    \n","\n","def read_splits(path):\n","  return pd.read_csv(path, converters={1:ast.literal_eval,\n","                                       2:ast.literal_eval})\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aLn-nvWGYoNP"},"source":["# Define Dataset Class for Images\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0pDmn7JEFsI"},"outputs":[],"source":["from tqdm import tqdm\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","\n","class NaturalImageDataset(Dataset):\n","  def __init__(self, data, augment = False):\n","\n","    dictator = 'features_label'    # What variable we use to upsample to match\n","    # upsample if augment\n","    if augment:\n","      sample_count = {}\n","      up_sampler = np.unique(data[dictator])\n","      for f in up_sampler:\n","          sample_count[f] = np.count_nonzero(data[dictator] == f)\n","\n","      maxcount = np.max(list(sample_count.values()))\n","      for f in up_sampler:\n","          gapnum = maxcount - sample_count[f]\n","          temp_df = data.iloc[np.random.choice(np.where(data[dictator] == f)[0], size = gapnum)]\n","          data = data.append(temp_df, ignore_index = True)\n","      \n","\n","    self.dataframe = data\n","    self.imgage_path = data[\"image_path\"].values\n","    self.labels = data[\"ts\"].values\n","    self.features = data[\"features\"].values\n","\n","    # transform image\n","    if augment:\n","      self.transform = transforms.Compose([\n","                                  transforms.Resize(256),\n","                                  transforms.CenterCrop(224),\n","                                  transforms.ToTensor(),\n","                                  transforms.RandomHorizontalFlip(p = 0.5),\n","                                  transforms.RandomVerticalFlip(p=0.5),\n","                                  transforms.ColorJitter(brightness = 0.1, contrast = 0.1),\n","                                  transforms.RandomAffine(degrees = 50, translate = (0.1, 0.1), scale = (0.9, 1.1)),\n","                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                              ])\n","    else:\n","      self.transform = transforms.Compose([\n","                                  transforms.Resize(256),\n","                                  transforms.CenterCrop(224),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                              ])\n","\n","\n","    self.images = [self.transform(Image.open(img_path)) for img_path in tqdm(data[\"image_path\"])]\n","\n","  def __len__(self):\n","    return (len(self.images))\n","\n","  def __getitem__(self, i):\n","    image = self.images[i]\n","    label = self.labels[i]\n","    feature = self.features[i]\n","    return image, torch.tensor(label, dtype=torch.long), torch.tensor(feature, dtype=torch.long)"]},{"cell_type":"markdown","metadata":{"id":"p1Ef68GVRMZK"},"source":["# MTLNet for dictator = \"features_labels\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuspTzacRRHY"},"outputs":[],"source":["# create the MTL network\n","from torch import nn\n","from torch import optim\n","import torchvision.models as models\n","\n","# create the MTL network\n","from torch import nn\n","from torch import optim\n","import torchvision.models as models\n","\n","class MTLNet(nn.Module):\n","    def __init__(self, num_classes, num_features):\n","        super(MTLNet, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.num_features = num_features\n","        \n","        # modify resnet\n","        base_net = models.resnet50(pretrained=True)\n","\n","        # Freeze all parameters of base network\n","        for param in base_net.parameters():\n","          param.requires_grad = False\n","\n","        # Unfreeze last 2 layers\n","        for name, param in base_net.named_parameters():\n","          if (name.startswith(\"layer4\")): #| (name.startswith(\"layer3\"))\n","            param.requires_grad = True\n","\n","        # Unfreeze all bn params\n","        for module in base_net.modules():\n","          if isinstance(module, nn.BatchNorm2d):\n","            for param in module.parameters():\n","              param.requires_grad = True\n","                \n","\n","        # get head infeatures\n","        head_in = base_net.fc.in_features\n","\n","        # strip out last layer\n","        base_net = nn.Sequential(*(list(base_net.children())[:-1]))\n","\n","        # construct the base model\n","        self.base_model = nn.Sequential(\n","            base_net\n","        )\n","\n","        # labels head part\n","        self.labels_head = nn.Sequential(\n","            nn.Dropout(p=0.8),\n","            nn.Linear(in_features = head_in, out_features = num_classes, bias=True)\n","        )\n","\n","        # labels head part\n","        self.features_head = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(in_features = head_in, out_features=num_features, bias=True)\n","        )\n","\n","    def forward(self, x):\n","\n","        # common part\n","        x = self.base_model(x)\n","        \n","        # flatten dimensions\n","        x = torch.flatten(x, 1) \n","\n","        # labels head part\n","        x_labels = self.labels_head(x)\n","\n","        # features head part\n","        \n","        x_features = self.features_head(x)\n","\n","        return x_labels, x_features\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"0Mvhdd2bc-h3"},"source":["# Define train loop for `MTLNet`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G35iulpxRBee"},"outputs":[],"source":["# Train the net\n","from HelperFunctions.project_utils import MTLTracker, plot_MTL_progress\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","\n","def train_MTL_net(net: MTLNet, criterion_labels, criterion_features,\n","                  optimizer, device, trainloader: DataLoader, validationloader: DataLoader = None,\n","                  validation_on: bool = False, num_epoch = 100, eval_every = 3,\n","                  plt_on: bool = False):\n","\n","\n","  scalars = [0.12, 1.1]\n","\n","  # Initialize trackers\n","  labels_tracker = MTLTracker()\n","  features_tracker = MTLTracker()\n","  total_tracker = MTLTracker(total=True)\n","\n","  # multilabel threshold\n","  threshold = 0.5          \n","\n","  for epoch in tqdm(range(num_epoch)):  \n","    \n","    # Train\n","    net.train()\n","\n","    for i, x in enumerate(trainloader):\n","      input_batch, labels, features = x\n","      input_batch, labels, features = input_batch.to(device), labels.to(device), features.to(device)\n","      \n","      output_labels, output_features = net(input_batch)\n","      \n","      # labels ------------------------------------------------------------\n","      target_labels = torch.argmax(labels,dim=1)\n","      probabilities = nn.functional.softmax(output_labels, dim = 1) \n","      preds_labels = torch.argmax(probabilities,dim=1)\n","\n","      loss_labels = criterion_labels(output_labels, target_labels)\n","      labels_tracker.batch_loss.append(loss_labels.item() / input_batch.size(0))\n","\n","      acc_labels = f1_score(target_labels.cpu(), preds_labels.cpu(), average='weighted')\n","      labels_tracker.batch_acc.append(acc_labels)\n","      # -------------------------------------------------------------------\n","\n","      # features ----------------------------------------------------------\n","      target_features = features\n","      probabilities = torch.sigmoid(output_features)\n","      pred_features = np.array(probabilities.cpu().detach().numpy() >= threshold, dtype=float)\n","\n","      loss_features = criterion_features(output_features, target_features.type(torch.float))\n","      features_tracker.batch_loss.append(loss_features.item() / input_batch.size(0))\n","\n","      acc_features = f1_score(target_features.cpu(), pred_features, average = \"samples\")\n","      features_tracker.batch_acc.append(acc_features)\n","      # -------------------------------------------------------------------\n","\n","      # calculate scaled total loss\n","      loss_total = loss_labels*scalars[0] + loss_features*scalars[1]\n","      total_tracker.batch_loss.append(loss_total.item() / input_batch.size(0))\n","      \n","      optimizer.zero_grad()\n","      loss_total.backward()\n","      optimizer.step()  \n","      \n","    # Update training values with batch results\n","\n","    labels_tracker.train_update(epoch)\n","    features_tracker.train_update(epoch)\n","    total_tracker.train_update(epoch)\n","\n","    # Validate\n","    if validation_on & ((epoch % eval_every == 0) | (epoch == num_epoch - 1)):\n","      net.eval() \n","      with torch.no_grad(): \n","\n","        for i, v in enumerate(validationloader):\n","    \n","          input_batch, labels, features = v\n","          input_batch, labels, features = input_batch.to(device), labels.to(device), features.to(device)\n","\n","          output_labels, output_features = net(input_batch)\n","\n","          # labels ------------------------------------------------------------\n","          target_labels = torch.argmax(labels,dim=1)\n","          probabilities = nn.functional.softmax(output_labels, dim = 1) \n","          preds_labels = torch.argmax(probabilities,dim=1)\n","          \n","          loss_labels = criterion_labels(output_labels, target_labels)\n","          labels_tracker.batch_loss.append(loss_labels.item() / input_batch.size(0))\n","\n","          acc_labels = f1_score(target_labels.cpu(), preds_labels.cpu(), average=\"weighted\")\n","          labels_tracker.batch_acc.append(acc_labels)\n","          # -------------------------------------------------------------------\n","\n","          # features ----------------------------------------------------------\n","          target_features = features\n","          probabilities = torch.sigmoid(output_features)\n","          pred_features = np.array(probabilities.cpu().detach().numpy() >= threshold, dtype=float)\n","\n","          loss_features = criterion_features(output_features, target_features.type(torch.float))\n","          features_tracker.batch_loss.append(loss_features.item() / input_batch.size(0))\n","\n","          acc_features = f1_score(target_features.cpu(), pred_features, average = \"samples\")\n","          features_tracker.batch_acc.append(acc_features)\n","          # -------------------------------------------------------------------\n","\n","          # calculate scaled total loss\n","          loss_total = loss_labels*scalars[0] + loss_features*scalars[1]\n","          \n","          total_tracker.batch_loss.append(loss_total.item() / input_batch.size(0))\n","\n","      labels_tracker.val_update(epoch)\n","      features_tracker.val_update(epoch)\n","      total_tracker.val_update(epoch)\n","\n","      # plot status\n","      if plt_on & ((epoch % eval_every == 0) | (epoch == num_epoch - 1)):\n","        plot_MTL_progress(labels_tracker, features_tracker, total_tracker, num_epoch)\n","\n","  return labels_tracker, features_tracker, total_tracker\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mTh0FBwLRpaF"},"source":["# Test `DomainNet`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPasJgbYxmei"},"outputs":[],"source":["\n","def test_MTL_net(net: MTLNet, testloader: DataLoader, device):\n","\n","  label_probs = []\n","  label_preds = []\n","  label_targets = []\n","\n","  feature_probs = []\n","  feature_preds = []\n","  feature_targets = []\n","\n","  threshold = 0.5            \n","\n","  for i, x in enumerate(testloader):\n","    input_batch, labels, features = x\n","    input_batch, labels, features = input_batch.to(device), labels.to(device), features.to(device)\n","\n","    output_labels, output_features = net(input_batch)\n","    tmp_batch_size = input_batch.size()[0]\n","\n","    # labels ------------------------------------------------------------\n","    target_labels = torch.argmax(labels,dim=1)\n","    prob_labels = nn.functional.softmax(output_labels, dim = 1) \n","    preds_labels = torch.argmax(prob_labels,dim=1)\n","\n","    label_probs = [*label_probs,*prob_labels.cpu().detach().numpy()]\n","    label_preds = [*label_preds,*preds_labels.cpu().detach().numpy()]\n","    label_targets = [*label_targets,*target_labels.cpu().detach().numpy()]\n","\n","    \n","    # -------------------------------------------------------------------\n","\n","    # features ----------------------------------------------------------\n","    target_features = features\n","    prob_features = torch.sigmoid(output_features)\n","    pred_features = np.array(prob_features.cpu().detach().numpy() >= threshold, dtype=float)\n","\n","    feature_probs = [*feature_probs, *prob_features.cpu().detach().numpy()]\n","    feature_preds = [*feature_preds, *pred_features]\n","    feature_targets = [*feature_targets, *target_features.cpu().detach().numpy()]\n","\n","\n","\n","  label_results = {\"probs\": label_probs,\n","                   \"preds\": label_preds,\n","                   \"targets\": label_targets}\n","\n","  feature_results = {\"probs\": feature_probs,\n","                      \"preds\": feature_preds,\n","                      \"targets\": feature_targets}\n","\n","  return label_results, feature_results\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"4iwmw1jAmj8D"},"source":["# K-Fold Cross Validation of `DomainNet`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222,"referenced_widgets":["f81f5f6a080e41fb9355b5bc7b69a044","30204b55179748129954227bfbb58cbf","b69571d5a18e49a7932c63be5619821a","0dbd332d56934b76b946bbc84b158b78","d0598e80c53a4252a79fe971369e1bda","e1de0ee7b5564a83aa4ae139f34cbca3","75b8d81c18da41b78d9d9d2c50b95863","8b386fdb986a49d9ad00884021aa635c","c2d162e275e54971a6a1e2db2ca18b63","207438f23e3548dfa9088dba3885360c","5ccd5c8760134f69baaa41ac1b20b785"]},"executionInfo":{"elapsed":4958313,"status":"ok","timestamp":1640000938615,"user":{"displayName":"thor thorsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07172622034323773422"},"user_tz":-60},"id":"EsW6JRgJ5oFE","outputId":"21aef041-a4fa-490a-8cdb-d119ebdef759"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2747/2747 [04:10<00:00, 10.97it/s]\n","100%|██████████| 90/90 [00:38<00:00,  2.34it/s]\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f81f5f6a080e41fb9355b5bc7b69a044","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [38:34<00:00, 46.30s/it]\n"]},{"name":"stdout","output_type":"stream","text":["K_fold/MTLNet_FINAL_kfold_NA_3.json\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2665/2665 [01:36<00:00, 27.62it/s]\n","100%|██████████| 90/90 [00:01<00:00, 71.33it/s]\n","100%|██████████| 50/50 [37:23<00:00, 44.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["K_fold/MTLNet_FINAL_kfold_NA_4.json\n"]}],"source":["from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn import utils\n","import json\n","\n","# move to GPU if possible\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Make a copy of labels dataset\n","data_df = data.copy()\n","\n","\n","NUM_FEATURES = len(data_df.features.to_list()[0])\n","NUM_CLASSES = len(data_df.diagnosis.unique())\n","BATCH_SIZE = 25\n","\n","\n","# USE TO CREATE NEW SPLIT \n","# k = 5\n","# skf = StratifiedKFold(n_splits=k)\n","# _x = np.zeros(len(data_df))\n","# _y = pd.DataFrame(data_df.ts.to_list()).idxmax(axis=1)\n","# save_splits(skf, _x, _y)\n","\n","# READ PRE-DEFINED SPLIT\n","splits = read_splits(\"K_fold/splits_FINAL_NA.csv\")\n","\n","k_epochs = 50\n","k_idx = 0\n","\n","#for train_idx, test_idx in skf.split(_x, _y):\n","for i, row in splits.iterrows():\n","\n","  if (i == 0) | (i == 1) | (i == 2):\n","    continue\n","\n","  train_idx = row.train\n","  test_idx = row.test\n","\n","  # Split train/test\n","  train_df = data_df.loc[train_idx]\n","  test_df = data_df.loc[test_idx]\n","\n","  # Load into DomainDataSet class\n","  trainset = NaturalImageDataset(train_df, augment=True)\n","  testset = NaturalImageDataset(test_df)\n","\n","  # Split into batches via DataLoader\n","  trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","  testloader = DataLoader(testset, batch_size=len(testset))\n","\n","\n","  # Initialize Net\n","  net = MTLNet(NUM_CLASSES, NUM_FEATURES)\n","  net.to(device)\n","\n","  # Get parameters to update\n","  params_to_update = []\n","  for name,param in net.named_parameters():\n","    if param.requires_grad == True:\n","      params_to_update.append(param)\n","    \n","\n","  class_weights = calc_multiclass_weights(trainset.dataframe, device)  # Adjust for data imbalance\n","  criterion_labels = nn.CrossEntropyLoss(weight = class_weights) #loss for labels\n","  \n","  feature_weights = calc_multilabel_weights(trainset.dataframe, device)\n","  criterion_features = nn.BCEWithLogitsLoss(pos_weight = feature_weights)\n","  \n","  optimizer = optim.SGD(params_to_update, lr = 0.00035, momentum = 0.9, weight_decay=0.05)\n","\n","  # Train net\n","  l_tracker, f_tracker, loss_tracker = train_MTL_net(net, criterion_labels, \n","                                                     criterion_features, optimizer,\n","                                                     device, trainloader,\n","                                                     num_epoch = k_epochs, plt_on = True)\n","  \n","  # Test net\n","  #l_results, f_results = test_MTL_net(net, testloader, device)\n","  net.eval()\n","  with torch.no_grad():\n","\n","    for _, x in enumerate(testloader):\n","      input_batch, labels, features = x\n","      input_batch, labels, features = input_batch.to(device), labels.to(device), features.to(device)\n","\n","      test_labels, test_features = net(input_batch)\n","      test_labels, test_features = test_labels.cpu().detach().numpy(), test_features.cpu().detach().numpy()\n","\n","\n","  run_info = {\n","      \"k_run\":                i,\n","      \"k_epochs\":             k_epochs,\n","\n","      \"train_idx\":            train_idx,\n","      \"test_idx\":             test_idx,\n","      \n","      \"batch_size\":           BATCH_SIZE,\n","      \"scalars\":              [0.12, 1.1],\n","\n","      \"labels_tracker\":       l_tracker.toJSON(),\n","      \"features_tracker\":     f_tracker.toJSON(),\n","      \"total_tracker\":        loss_tracker.toJSON(),\n","\n","      \"test_labels\":          test_labels.tolist(),\n","      \"test_labels_targets\":  labels.cpu().detach().numpy().tolist(),\n","      \n","      \"test_features\":        test_features.tolist(),\n","      \"test_features_targets\":features.cpu().detach().numpy().tolist()\n","  }\n","\n","  save_name = f\"K_fold/MTLNet_FINAL_kfold_NA_{i}.json\"\n","  with open(save_name, \"w\") as f:\n","    print(save_name)\n","    json.dump(run_info,f)\n","\n","  k_idx += 1\n","\n","    \n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"TH MTL net kfold (NA).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0dbd332d56934b76b946bbc84b158b78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2d162e275e54971a6a1e2db2ca18b63","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b386fdb986a49d9ad00884021aa635c","value":102530333}},"207438f23e3548dfa9088dba3885360c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30204b55179748129954227bfbb58cbf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ccd5c8760134f69baaa41ac1b20b785":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75b8d81c18da41b78d9d9d2c50b95863":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b386fdb986a49d9ad00884021aa635c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b69571d5a18e49a7932c63be5619821a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75b8d81c18da41b78d9d9d2c50b95863","placeholder":"​","style":"IPY_MODEL_e1de0ee7b5564a83aa4ae139f34cbca3","value":"100%"}},"c2d162e275e54971a6a1e2db2ca18b63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0598e80c53a4252a79fe971369e1bda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ccd5c8760134f69baaa41ac1b20b785","placeholder":"​","style":"IPY_MODEL_207438f23e3548dfa9088dba3885360c","value":" 97.8M/97.8M [00:00&lt;00:00, 118MB/s]"}},"e1de0ee7b5564a83aa4ae139f34cbca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f81f5f6a080e41fb9355b5bc7b69a044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b69571d5a18e49a7932c63be5619821a","IPY_MODEL_0dbd332d56934b76b946bbc84b158b78","IPY_MODEL_d0598e80c53a4252a79fe971369e1bda"],"layout":"IPY_MODEL_30204b55179748129954227bfbb58cbf"}}}}},"nbformat":4,"nbformat_minor":0}
